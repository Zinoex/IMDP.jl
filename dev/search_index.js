var documenterSearchIndex = {"docs":
[{"location":"reference/data/#Data-formats","page":"Data Storage","title":"Data formats","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"CurrentModule = IntervalMDP.Data","category":"page"},{"location":"reference/data/#PRISM","page":"Data Storage","title":"PRISM","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"write_prism_file\nread_prism_file","category":"page"},{"location":"reference/data/#IntervalMDP.Data.write_prism_file","page":"Data Storage","title":"IntervalMDP.Data.write_prism_file","text":"write_prism_file(path_without_file_ending, problem)\n\nWrite the files required by PRISM explicit engine/format to \n\npath_without_file_ending.sta (states),\npath_without_file_ending.lab (labels),\npath_without_file_ending.tra (transitions), and\npath_without_file_ending.pctl (properties).\n\nIf the specification is a reward optimization problem, then a state rewards file .srew is also written.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_prism_file","page":"Data Storage","title":"IntervalMDP.Data.read_prism_file","text":"read_prism_file(path_without_file_ending)\n\nRead PRISM explicit file formats and pctl file, and return a Problem including system and specification.\n\nSee PRISM Explicit Model Files for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#bmdp-tool","page":"Data Storage","title":"bmdp-tool","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_bmdp_tool_file\nwrite_bmdp_tool_file","category":"page"},{"location":"reference/data/#IntervalMDP.Data.read_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.read_bmdp_tool_file","text":"read_bmdp_tool_file(path)\n\nRead a bmdp-tool transition probability file and return an IntervalMarkovDecisionProcess and a list of terminal states. From the file format, it is not clear if the desired reachability verification if the reachability specification is finite or infinite horizon, the satisfaction_mode is pessimistic or optimistic, or if the actions should minimize or maximize the probability of reachability.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_bmdp_tool_file","page":"Data Storage","title":"IntervalMDP.Data.write_bmdp_tool_file","text":"write_bmdp_tool_file(path, problem::Problem)\n\nWrite a bmdp-tool transition probability file for the given an IMDP and a reachability specification. The file will not contain enough information to specify a reachability specification. The remaining parameters are rather command line arguments.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, spec::Specification)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovProcess, prop::AbstractReachability)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovDecisionProcess, terminal_states::Vector{T})\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.jl","page":"Data Storage","title":"IntervalMDP.jl","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_intervalmdp_jl\nread_intervalmdp_jl_model\nread_intervalmdp_jl_spec\nwrite_intervalmdp_jl_model\nwrite_intervalmdp_jl_spec","category":"page"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl","text":"read_intervalmdp_jl(model_path, spec_path)\n\nRead an IntervalMDP.jl data file and return an IntervalMarkovDecisionProcess or IntervalMarkovChain and a list of terminal states. \n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_model","text":"read_intervalmdp_jl_model(model_path)\n\nRead an IntervalMarkovDecisionProcess or IntervalMarkovChain from an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.read_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.read_intervalmdp_jl_spec","text":"read_intervalmdp_jl_spec(spec_path)\n\nRead a Specification from an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_model","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_model","text":"write_intervalmdp_jl_model(model_path, mdp_or_mc)\n\nWrite an IntervalMarkovDecisionProcess or IntervalMarkovChain to an IntervalMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IntervalMDP.Data.write_intervalmdp_jl_spec","page":"Data Storage","title":"IntervalMDP.Data.write_intervalmdp_jl_spec","text":"write_intervalmdp_jl_spec(spec_path, spec::Specification)\n\nWrite a Specification to an IntervalMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"data/#Data-storage-formats","page":"Data formats","title":"Data storage formats","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl supports reading and writing data in various formats, namely PRISM explicit format, bmdp-tool, and our own format (model and specification). To justify introducing another standard (see relevant XKCD), note that the PRISM explicit format and the bmdp-tool format are all written in ASCII, which is very inefficient in terms of storage space (especially for storing floating point numbers) and parsing time. We propose a binary format for the most storage-intensive part of the data, namely the transition probabilities, and use JSON for the specification, which is human- and machine-readable and widely used.","category":"page"},{"location":"data/#PRISM","page":"Data formats","title":"PRISM","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl supports reading and writing PRISM [1] explicit data format.  The data format is split into 4 different files, one for the states, one for the labels, one for the transition probabilities, and one for the specification. Therefore, our interface for reading PRISM files takes the path without file ending and adds the appropriate ending to each of the four files.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nproblem = read_prism_file(path_without_file_ending)\n\n# Write\nwrite_prism_file(path_without_file_ending, problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"The problem structure contains both the \\gls{imdp} and the specification including whether to synthesize a maximizing or minimizing strategy and whether to use an optimistic or pessimistic adversary.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"PRISM uses 4 different ASCII-encoded files to store the explicit representation of the system: '.sta' (states), '.lab' (labels), '.tra' (transitions), and '.pctl' (property). In the tables below, we list the format for each file. The extended details of the PRISM explicit file format can be found in the appendix of the PRISM manual.","category":"page"},{"location":"data/#States-.sta","page":"Data formats","title":"States .sta","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line The first line containing symbolically (v1, v2, ..., vn) is a list of n variables in the model.\nm lines where m is the number of states Each line contains i:(v1, v2, ..., vn) where i is the index of the state and (v1, v2, ..., vn)` is an assignment of values to the variables in the model. Indices are zero-indexed.","category":"page"},{"location":"data/#Labels-.lab","page":"Data formats","title":"Labels .lab","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line Contains a space-separated list of labels with index i=\"label\". The first two must be 0=\"init\" 1=\"deadlock\".\nAll remaining lines Contains i: j1 j2 j3 ... where i is a state index and j1 j2 j3 ... are space-separated indices of labels associated with state i.","category":"page"},{"location":"data/#Transitions-.tra","page":"Data formats","title":"Transitions .tra","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line num_states num_choices num_transitions where num_state must match the number in the state file.\nFollowing num_transitions lines A list of transition probabilities with the format src_idx act_idx dest_idx [p_lower,p_upper] action.","category":"page"},{"location":"data/#Property-.pctl","page":"Data formats","title":"Property .pctl","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line PRISM property specification","category":"page"},{"location":"data/#bmdp-tool","page":"Data formats","title":"bmdp-tool","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"bmdp-tool data format is similar to the PRISM explicit format transition probability files, where transition probabilities are stored line-by-line with source, action, destination, and probability bounds in ASCII. Key differences include no explicit listing of states, the fact that it only supports reachability properties, and that terminal states are listed directly in the transition probability file. As a result, bmdp-tool data format is a single file. This format lacks information about whether the reachability is finite or infinite time, and hence the reader only returns the set of terminal states.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nimdp, terminal_states = read_bmdp_tool_file(path)\n\n# Write\nwrite_bmdp_tool_file(path, problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"bmdp-tool uses only one ASCII file with the following format:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Number of lines Description\nFirst line num_states.\nSecond line num_actions (not to be confused with num_choices of PRISM).\nThird line num_terminal.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nThe following num_terminal lines Indices (zero-indexed) of terminal states, one per line.\nAll remaining lines A list of transition probabilities with the format src_idx act_idx dest_idx p_lower p_upper.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"terminology: Choices vs actions\nIn PRISM, the number of choices which is listed in the transition file is the sum of the number of feasible actions in each state. In bmdp-tool, the number of actions is the total number of different actions in the model, i.e. in each state up to num_actions may be feasible. This is a subtle difference, but it is important to be aware of as the parsing in either tool requires the right number to be specified.","category":"page"},{"location":"data/#IntervalMDP.jl","page":"Data formats","title":"IntervalMDP.jl","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"IntervalMDP.jl also supports a different binary format based on NetCDF to store transition probabilities. We use JSON to store the specification, as storage space for the specification is much less a concern, and because JSON is a widely used, human-readable, file format.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"# Read\nimdp = read_intervalmdp_jl_model(model_path)\nspec = read_intervalmdp_jl_spec(spec_path)\nproblem = Problem(imdp, spec)\n\nproblem = read_intervalmdp_jl(model_path, spec_path)\n\n# Write\nwrite_intervalmdp_jl_model(model_path, imdp_or_problem)\nwrite_intervalmdp_jl_spec(spec_path, spec_or_problem)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"The new format proposed uses netCDF, which is based on HDF5 underlying, to store transition probabilities, and a JSON file to store the specification. Transition probabilities are stored in CSC-format, which is unfortunately not natively stored in netCDF, nor any widely available format. Therefore, we store the following attributes and variables in the netCDF file:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Global attributes:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"num_states\nmodel (either imc or imdp)\nformat (assert sparse_csc)\nrows (assert to)\ncols (assert from if model is imc and from/action if model is imdp)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"Variables:","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"lower_colptr (integer)\nlower_rowval (integer)\nlower_nzval (floating point)\nupper_colptr (integer)\nupper_rowval (integer)\nupper_nzval (floating point)\ninitial_states (integer)\nstateptr (integer, only for imdp)\naction_vals (any netCDF supported type, only for imdp)","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"We store the specification in a JSON format where the structure depends on the type of specification. For a reachability-like specification, the specification is the following format","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"{\n    \"property\": {\n        \"type\": <\"reachability\"|\"reach-avoid\">,\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reach\": [<state_index:positive int>],\n        \"avoid\": [<state_index:positive int>]\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"For a finite horizon property, eps is excluded, and similarly for an infinite horizon property, time\\_horizon is excluded.  For a proper reachability property, the avoid-field is excluded.","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"If we instead want to optimize a reward, the format is the following","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"{\n    \"property\": {\n        \"type\": \"reward\",\n        \"infinite_time\": <true|false>,\n        \"time_horizon\": <positive int>,\n        \"eps\": <positive float>,\n        \"reward\": [<reward_per_state_index:float>]\n        \"discount\" <float:0-1>\n    },\n    \"satisfaction_mode\": <\"pessimistic\"|\"optimistic\">,\n    \"strategy_mode\": <\"minimize\"|\"maximize\">\n}","category":"page"},{"location":"data/","page":"Data formats","title":"Data formats","text":"[1] Kwiatkowska, Marta, Gethin Norman, and David Parker. \"PRISM 4.0: Verification of probabilistic real-time systems.\" Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23. Springer Berlin Heidelberg, 2011.","category":"page"},{"location":"reference/systems/#System-representation","page":"Systems","title":"System representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovProcess","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovProcess","page":"Systems","title":"IntervalMDP.IntervalMarkovProcess","text":"IntervalMarkovProcess{P <: IntervalProbabilities}\n\nAn abstract type for interval Markov processes including IntervalMarkovChain and IntervalMarkovDecisionProcess.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#Markov-chain","page":"Systems","title":"Markov chain","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovChain\ntransition_prob(mc::IntervalMarkovChain)\nnum_states(mc::IntervalMarkovChain)\ninitial_states(mc::IntervalMarkovChain)","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovChain","page":"Systems","title":"IntervalMDP.IntervalMarkovChain","text":"IntervalMarkovChain{P <: IntervalProbabilities, T <: Integer, VT <: AbstractVector{T}}\n\nA type representing Interval Markov Chains (IMC), which are Markov Chains with uncertainty in the form of intervals on the transition probabilities.\n\nFormally, let (S S_0 barP underbarP) be an interval Markov chain, where S is the set of states, S_0 subset S is a set of initial states, and barP  mathbbR^S times S and underbarP  mathbbR^S times S are the upper and lower bound transition probability matrices prespectively. Then the IntervalMarkovChain type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, and initial_states is the set of initial state S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\ntransition_prob::P: interval on transition probabilities.\ninitial_states::VT: initial states.\nnum_states::T: number of states.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\nmc = IntervalMarkovChain(prob)\n# or\ninitial_states = [1, 2, 3]\nmc = IntervalMarkovChain(prob, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.transition_prob-Tuple{IntervalMarkovChain}","page":"Systems","title":"IntervalMDP.transition_prob","text":"transition_prob(s::IntervalMarkovChain)\n\nReturn the interval on transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_states-Tuple{IntervalMarkovChain}","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(s::IntervalMarkovChain)\n\nReturn the number of states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.initial_states-Tuple{IntervalMarkovChain}","page":"Systems","title":"IntervalMDP.initial_states","text":"initial_states(s::IntervalMarkovChain)\n\nReturn the initial states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Markov-decision-process","page":"Systems","title":"Markov decision process","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovDecisionProcess\ntransition_prob(mdp::IntervalMarkovDecisionProcess)\nnum_states(mdp::IntervalMarkovDecisionProcess)\ninitial_states(mdp::IntervalMarkovDecisionProcess)\nactions(mdp::IntervalMarkovDecisionProcess)\nnum_choices(mdp::IntervalMarkovDecisionProcess)","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalMarkovDecisionProcess","page":"Systems","title":"IntervalMDP.IntervalMarkovDecisionProcess","text":"IntervalMarkovDecisionProcess{\n    P <: IntervalProbabilities,\n    T <: Integer,\n    VT <: AbstractVector{T},\n    VI <: AbstractVector{T},\n    VA <: AbstractVector,\n}\n\nA type representing Interval Markov Decision Processes (IMDP), which are Markov Decision Processes with uncertainty in the form of intervals on the transition probabilities.\n\nFormally, let (S S_0 A barP underbarP) be an interval Markov decision processes, where S is the set of states, S_0 subset S is the set of initial states, A is the set of actions, and barP  A to mathbbR^S times S and underbarP  A to mathbbR^S times S are functions representing the upper and lower bound transition probability matrices prespectively for each action. Then the IntervalMarkovDecisionProcess type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, action_vals contains the actions available in each state, and initial_states is the set of initial states S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\ntransition_prob::P: interval on transition probabilities where columns represent source/action pairs and rows represent target states.\nstateptr::VT: pointer to the start of each source state in transition_prob (i.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition   probability matrix for source state j) in the style of colptr for sparse matrices in CSC format.\naction_vals::VA: actions available in each state. Can be any eltype.\ninitial_states::VI: initial states.\nnum_states::T: number of states.\n\nExamples\n\ntransition_probs = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.1 0.2 0.0\n        0.1 0.3 0.2 0.3 0.0\n        0.2 0.1 0.3 0.4 1.0\n    ],\n    upper = [\n        0.5 0.7 0.6 0.6 0.0\n        0.6 0.5 0.5 0.5 0.0\n        0.7 0.3 0.4 0.4 1.0\n    ],\n)\n\nstateptr = [1, 3, 5, 6]\nactions = [\"a1\", \"a2\", \"a1\", \"a2\", \"sinking\"]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, stateptr, actions, initial_states)\n\nThere is also a constructor for IntervalMarkovDecisionProcess where the transition probabilities are given as a list of  mappings from actions to transition probabilities for each source state.\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.transition_prob-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.transition_prob","text":"transition_prob(s::IntervalMarkovDecisionProcess)\n\nReturn the interval on transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_states-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.num_states","text":"num_states(s::IntervalMarkovDecisionProcess)\n\nReturn the number of states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.initial_states-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.initial_states","text":"initial_states(s::IntervalMarkovDecisionProcess)\n\nReturn the initial states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.actions-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.actions","text":"actions(s::IntervalMarkovDecisionProcess)\n\nReturn a vector of actions (choices in PRISM terminology).\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_choices-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IntervalMDP.num_choices","text":"num_choices(s::IntervalMarkovDecisionProcess)\n\nReturn the sum of the number of actions available in each state sum_j mathrmnum_actions(s_j).\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Probability-representation","page":"Systems","title":"Probability representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalProbabilities\nlower(s::IntervalProbabilities)\nupper(s::IntervalProbabilities)\ngap(s::IntervalProbabilities)\nsum_lower(s::IntervalProbabilities)\nnum_source(s::IntervalProbabilities)\nnum_target(s::IntervalProbabilities)\naxes_source(s::IntervalProbabilities)\naxes_target(s::IntervalProbabilities)","category":"page"},{"location":"reference/systems/#IntervalMDP.IntervalProbabilities","page":"Systems","title":"IntervalMDP.IntervalProbabilities","text":"IntervalProbabilities{R, VR <: AbstractVector{R}, MR <: AbstractMatrix{R}}\n\nA matrix pair to represent the lower and upper bound transition probabilities from a source state or source/action pair to a target state. The matrices can be Matrix{R} or SparseMatrixCSC{R}, or their CUDA equivalents. For memory efficiency, it is recommended to use sparse matrices.\n\nThe columns represent the source and the rows represent the target, as if the probability matrix was a linear transformation. Mathematically, let P be the probability matrix. Then P_ij represents the probability of transitioning from state j (or with state/action pair j) to state i. Due to the column-major format of Julia, this is also a more efficient representation (in terms of cache locality).\n\nThe lower bound is explicitly stored, while the upper bound is computed from the lower bound and the gap. This choice is  because it simplifies repeated probability assignment using O-maximization [1].\n\nFields\n\nlower::MR: The lower bound transition probabilities from a source state or source/action pair to a target state.\ngap::MR: The gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\nsum_lower::VR: The sum of lower bound transition probabilities from a source state or source/action pair to all target states.\n\nExamples\n\ndense_prob = IntervalProbabilities(;\n    lower = [0.0 0.5; 0.1 0.3; 0.2 0.1],\n    upper = [0.5 0.7; 0.6 0.5; 0.7 0.3],\n)\n\nsparse_prob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IntervalMDP.lower-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.lower","text":"lower(s::IntervalProbabilities)\n\nReturn the lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.upper-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.upper","text":"upper(s::IntervalProbabilities)\n\nReturn the upper bound transition probabilities from a source state or source/action pair to a target state.\n\nnote: Note\nIt is not recommended to use this function for the hot loop of O-maximization, because it is not just an accessor and requires allocation and computation.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.gap-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.gap","text":"gap(s::IntervalProbabilities)\n\nReturn the gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.sum_lower-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.sum_lower","text":"sum_lower(s::IntervalProbabilities)\n\nReturn the sum of lower bound transition probabilities from a source state or source/action pair to all target states. This is useful in efficiently implementing O-maximization, where we start with a lower bound probability assignment and iteratively, according to the ordering, adding the gap until the sum of probabilities is 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_source-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.num_source","text":"num_source(s::IntervalProbabilities)\n\nReturn the number of source states or source/action pairs.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.num_target-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.num_target","text":"num_target(s::IntervalProbabilities)\n\nReturn the number of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.axes_source-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.axes_source","text":"axes_source(s::IntervalProbabilities)\n\nReturn the valid range of indices for the source states or source/action pairs.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IntervalMDP.axes_target-Tuple{IntervalProbabilities}","page":"Systems","title":"IntervalMDP.axes_target","text":"axes_target(s::IntervalProbabilities)\n\nReturn the valid range of indices for the target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Problem","page":"Specifications","title":"Problem","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"Problem\nsystem\nspecification\nSpecification\nsystem_property\nProperty\nsatisfaction_mode\nSatisfactionMode\nstrategy_mode\nStrategyMode","category":"page"},{"location":"reference/specifications/#IntervalMDP.Problem","page":"Specifications","title":"IntervalMDP.Problem","text":"Problem{S <: IntervalMarkovProcess, F <: Specification}\n\nA problem is a tuple of an interval Markov process and a specification.\n\nFields\n\nsystem::S: interval Markov process.\nspec::F: specification (either temporal logic or reachability-like).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system","page":"Specifications","title":"IntervalMDP.system","text":"system(prob::Problem)\n\nReturn the system of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.specification","page":"Specifications","title":"IntervalMDP.specification","text":"specification(prob::Problem)\n\nReturn the specification of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.Specification","page":"Specifications","title":"IntervalMDP.Specification","text":"Specification{F <: Property}\n\nA specfication is a property together with a satisfaction mode and a strategy mode.  The satisfaction mode is either Optimistic or Pessimistic. See SatisfactionMode for more details. The strategy  mode is either Maxmize or Minimize. See StrategyMode for more details.\n\nFields\n\nprop::F: verification property (either temporal logic or reachability-like).\nsatisfaction::SatisfactionMode: satisfaction mode (either optimistic or pessimistic). Default is pessimistic.\nstrategy::StrategyMode: strategy mode (either maximize or minimize). Default is maximize.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.system_property","page":"Specifications","title":"IntervalMDP.system_property","text":"system_property(spec::Specification)\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.Property","page":"Specifications","title":"IntervalMDP.Property","text":"Property\n\nSuper type for all system Property\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.satisfaction_mode","page":"Specifications","title":"IntervalMDP.satisfaction_mode","text":"satisfaction_mode(spec::Specification)\n\nReturn the satisfaction mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.SatisfactionMode","page":"Specifications","title":"IntervalMDP.SatisfactionMode","text":"SatisfactionMode\n\nWhen computing the satisfaction probability of a property over an interval Markov process, be it IMC or IMDP, the desired satisfaction probability to verify can either be Optimistic or Pessimistic. That is, upper and lower bounds on the satisfaction probability within the probability uncertainty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.strategy_mode","page":"Specifications","title":"IntervalMDP.strategy_mode","text":"strategy_mode(spec::Specification)\n\nReturn the strategy mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IntervalMDP.StrategyMode","page":"Specifications","title":"IntervalMDP.StrategyMode","text":"StrategyMode\n\nWhen computing the satisfaction probability of a property over an IMDP, the strategy can either maximize or minimize the satisfaction probability (wrt. the satisfaction mode).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Temporal-logic","page":"Specifications","title":"Temporal logic","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"IntervalMDP.AbstractTemporalLogic\n\nLTLFormula\nIntervalMDP.isfinitetime(spec::LTLFormula)\nLTLfFormula\nIntervalMDP.isfinitetime(spec::LTLfFormula)\ntime_horizon(spec::LTLfFormula)\nPCTLFormula","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractTemporalLogic","page":"Specifications","title":"IntervalMDP.AbstractTemporalLogic","text":"AbstractTemporalLogic\n\nSuper type for temporal logic property\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.LTLFormula","page":"Specifications","title":"IntervalMDP.LTLFormula","text":"LTLFormula\n\nLinear Temporal Logic (LTL) property (first-order logic + next and until operators) [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] Vardi, M.Y. (1996). An automata-theoretic approach to linear temporal logic. In: Moller, F., Birtwistle, G. (eds) Logics for Concurrency. Lecture Notes in Computer Science, vol 1043. Springer, Berlin, Heidelberg.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{LTLFormula}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::LTLFormula)\n\nReturn false for an LTL formula. LTL formulas are not finite time property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.LTLfFormula","page":"Specifications","title":"IntervalMDP.LTLfFormula","text":"LTLfFormula\n\nAn LTL formula over finite traces [1]. See LTLFormula for the structure of LTL formulas. Let ϕ denote the formula, M denote an interval Markov process, and H the time horizon. Then compute M  ϕ within traces of length H.\n\nFields\n\nformula::String: LTL formula\ntime_horizon::T: Time horizon of the finite traces \n\n[1] Giuseppe De Giacomo and Moshe Y. Vardi. 2013. Linear temporal logic and linear dynamic logic on finite traces. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (IJCAI '13). AAAI Press, 854–860.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{LTLfFormula}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(spec::LTLfFormula)\n\nReturn true for an LTLf formula. LTLf formulas are specifically over finite traces.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{LTLfFormula}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(spec::LTLfFormula)\n\nReturn the time horizon of an LTLf formula.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.PCTLFormula","page":"Specifications","title":"IntervalMDP.PCTLFormula","text":"PCTLFormula\n\nA Probabilistic Computation Tree Logic (PCTL) formula [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Reachability","page":"Specifications","title":"Reachability","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachability\n\nFiniteTimeReachability\nIntervalMDP.isfinitetime(spec::FiniteTimeReachability)\nterminal_states(spec::FiniteTimeReachability)\nreach(spec::FiniteTimeReachability)\ntime_horizon(spec::FiniteTimeReachability)\n\nInfiniteTimeReachability\nIntervalMDP.isfinitetime(spec::InfiniteTimeReachability)\nterminal_states(spec::InfiniteTimeReachability)\nreach(spec::InfiniteTimeReachability)\nconvergence_eps(spec::InfiniteTimeReachability)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReachability","page":"Specifications","title":"IntervalMDP.AbstractReachability","text":"AbstractReachability\n\nSuper type for all reachability-like property.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachability","page":"Specifications","title":"IntervalMDP.FiniteTimeReachability","text":"FiniteTimeReachability{T <: Integer, VT <: AbstractVector{T}}\n\nFinite time reachability specified by a set of target/terminal states and a time horizon.  That is, if T is the set of target states and H is the time horizon, compute ℙ(k = 0H s_k  T).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachability)\n\nReturn true for FiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(spec::FiniteTimeReachability)\n\nReturn the set of terminal states of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"terminal_states(prop::FiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a finite time reachability prop. This is equivalent for terminal_states(prop::FiniteTimeReachability) for a regular reachability property. See FiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachability)\n\nReturn the time horizon of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachability","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachability","text":"InfiniteTimeReachability{R <: Real, T <: Integer, VT <: AbstractVector{T}}\n\nInfiniteTimeReachability is similar to FiniteTimeReachability except that the time horizon is infinite. The convergence threshold is that the largest value of the most recent Bellman residual is less than eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachability)\n\nReturn false for InfiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachability)\n\nReturn the set of terminal states of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a infinite time reachability property. This is equivalent for terminal_states(prop::InfiniteTimeReachability) for a regular reachability property. See InfiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachability)\n\nReturn the convergence threshold of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reach-avoid","page":"Specifications","title":"Reach-avoid","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachAvoid\n\nFiniteTimeReachAvoid\nIntervalMDP.isfinitetime(spec::FiniteTimeReachAvoid)\nterminal_states(spec::FiniteTimeReachAvoid)\nreach(spec::FiniteTimeReachAvoid)\navoid(spec::FiniteTimeReachAvoid)\ntime_horizon(spec::FiniteTimeReachAvoid)\n\nInfiniteTimeReachAvoid\nIntervalMDP.isfinitetime(spec::InfiniteTimeReachAvoid)\nterminal_states(spec::InfiniteTimeReachAvoid)\nreach(spec::InfiniteTimeReachAvoid)\navoid(spec::InfiniteTimeReachAvoid)\nconvergence_eps(spec::InfiniteTimeReachAvoid)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReachAvoid","page":"Specifications","title":"IntervalMDP.AbstractReachAvoid","text":"AbstractReachAvoid\n\nA property of reachability that includes a set of states to avoid.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.FiniteTimeReachAvoid","text":"FiniteTimeReachAvoid{T <: Integer, VT <: AbstractVector{T}}\n\nFinite time reach-avoid specified by a set of target/terminal states, a set of avoid states, and a time horizon. That is, if T is the set of target states, A is the set of states to avoid, and H is the time horizon, compute ℙ(k = 0H s_k  T and k = 0k s_k  A).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachAvoid)\n\nReturn true for FiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::FiniteTimeReachAvoid)\n\nReturn the set of terminal states of a finite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::FiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::FiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachAvoid)\n\nReturn the time horizon of a finite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReachAvoid","page":"Specifications","title":"IntervalMDP.InfiniteTimeReachAvoid","text":"InfiniteTimeReachAvoid{R <: Real, T <: Integer, VT <: AbstractVector{T}}\n\nInfiniteTimeReachAvoid is similar to FiniteTimeReachAvoid except that the time horizon is infinite.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachAvoid)\n\nReturn false for InfiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.terminal_states-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachAvoid)\n\nReturn the set of terminal states of an infinite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reach-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.reach","text":"reach(prop::InfiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.avoid-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.avoid","text":"avoid(prop::InfiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachAvoid)\n\nReturn the convergence threshold of an infinite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reward-specification","page":"Specifications","title":"Reward specification","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReward\n\nFiniteTimeReward\nIntervalMDP.isfinitetime(spec::FiniteTimeReward)\nreward(spec::FiniteTimeReward)\ndiscount(spec::FiniteTimeReward)\ntime_horizon(spec::FiniteTimeReward)\n\nInfiniteTimeReward\nIntervalMDP.isfinitetime(spec::InfiniteTimeReward)\nreward(spec::InfiniteTimeReward)\ndiscount(spec::InfiniteTimeReward)\nconvergence_eps(spec::InfiniteTimeReward)","category":"page"},{"location":"reference/specifications/#IntervalMDP.AbstractReward","page":"Specifications","title":"IntervalMDP.AbstractReward","text":"AbstractReward{R <: Real}\n\nSuper type for all reward specifications.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.FiniteTimeReward","page":"Specifications","title":"IntervalMDP.FiniteTimeReward","text":"FiniteTimeReward{R <: Real, T <: Integer, VR <: AbstractVector{R}}\n\nFiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor. The time horizon is finite, so the discount factor is optional and  the optimal policy will be time-varying.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReward)\n\nReturn true for FiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.time_horizon-Tuple{FiniteTimeReward}","page":"Specifications","title":"IntervalMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReward)\n\nReturn the time horizon of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.InfiniteTimeReward","page":"Specifications","title":"IntervalMDP.InfiniteTimeReward","text":"InfiniteTimeReward{R <: Real, VR <: AbstractVector{R}}\n\nInfiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor for guaranteed convergence. The time horizon is infinite, so the optimal policy will be stationary.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IntervalMDP.isfinitetime-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReward)\n\nReturn false for InfiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.reward-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.discount-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IntervalMDP.convergence_eps-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IntervalMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReward)\n\nReturn the convergence threshold of an infinite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/certification/#Certification","page":"Certification & synthesis","title":"Certification","text":"","category":"section"},{"location":"reference/certification/","page":"Certification & synthesis","title":"Certification & synthesis","text":"satisfaction_probability\ncontrol_synthesis","category":"page"},{"location":"reference/certification/#IntervalMDP.satisfaction_probability","page":"Certification & synthesis","title":"IntervalMDP.satisfaction_probability","text":"satisfaction_probability(problem::Problem{<:IntervalMarkovProcess, <:AbstractReachability})\n\nCompute the probability of satisfying the reachability-like specification from the initial state.\n\n\n\n\n\n","category":"function"},{"location":"reference/certification/#IntervalMDP.control_synthesis","page":"Certification & synthesis","title":"IntervalMDP.control_synthesis","text":"control_synthesis(problem::Problem{<:IntervalMarkovDecisionProcess})\n\nCompute the optimal control policy for the given problem (system + specification). If the specification is finite time, then the policy is time-varying, with the returned policy being in step order (i.e., the first element of the returned vector is the policy for the first time step). If the specification is infinite time, then the policy is stationary and only a single vector of length num_states(system) is returned.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"Index","title":"Index","text":"","category":"page"},{"location":"theory/#Theory","page":"Theory","title":"Theory","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Interval Markov Decision Processes (IMDPs), also called bounded-parameter MDPs [1], are a generalization of MDPs, where the transition probabilities, given source state and action, are not known exactly, but they are constrained to be in some probability interval.  Formally, an IMDP M is a tuple M = (S S_0 A overlineP underlineP), where","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"S is a finite set of states,\nS_0 subseteq S is a set of initial states,\nA is a finite set of actions,\nunderlineP S times A times S  to 01 is a function, where underlineP(sas) defines the lower bound of the transition probability from state sin S (source) to state sin S (destination) under action a in A,\noverlineP S times A times S to 01 is a function, where overlineP(sas) defines the upper bound of the transition probability from state sin S to state sin S under action a in A.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For each state-action pair (sa) in S times A, it holds that sum_sin S underlineP(sas) leq 1 leq sum_sin S overlineP(sas) and a transition probability distribution p_saSto01 is called feasible if underlineP(sas) leq p_sa(s) leq overlineP(sas) for all destinations sin S. The set of all feasible distributions for the state-action pair (sa) is denoted by Gamma_sa.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"A path of an IMDP is a sequence of states and actions omega = (s_0a_0)(s_1a_1)dots, where (s_ia_i)in S times A. We denote by omega(k) = s_k the state of the path at time k in mathbbN^0 and by Omega the set of all paths.   A strategy or policy for an IMDP is a function pi that assigns an action to a given state of an IMDP. Time-dependent strategies are functions from state and time step to an action, i.e. pi Stimes mathbbN^0 to A. If pi does not depend on time and solely depends on the current state, it is called a stationary strategy. Similar to a strategy, an adversary eta is a function that assigns a feasible distribution to a given state. Given a strategy and an adversary, an IMDP collapses to a finite Markov chain.","category":"page"},{"location":"theory/#Reachability","page":"Theory","title":"Reachability","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"In this formal framework, we can describe computing reachability given a target set G and a horizon K in mathbbN cup infty as the following objective ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"mathopoptlimits_pi^pi  mathopoptlimits_eta^eta  mathbbP_pieta leftomega in Omega  exists k in 0K  omega(k)in G  right","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"where mathopopt^pimathopopt^eta in min max and mathbbP_pieta  is the probability of the Markov chain induced by strategy pi and adversary eta. When mathopopt^eta = min, the solution is called optimal pessimistic probability (or reward), and conversely is called optimal optimistic probability (or reward) when mathopopt^eta = max. The choice of the min/max for the action and pessimistic/optimistic probability depends on the application. ","category":"page"},{"location":"theory/#Discounted-reward","page":"Theory","title":"Discounted reward","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"Discounted reward is similar to reachability but instead of a target set, we have a reward function r S to mathbbR and a discount factor gamma in (0 1). The objective is then","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"mathopoptlimits_pi^pi  mathopoptlimits_eta^eta  mathbbE_pieta leftsum_k=0^K gamma^k r(omega(k)) right","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"[1] Givan, Robert, Sonia Leach, and Thomas Dean. \"Bounded-parameter Markov decision processes.\" Artificial Intelligence 122.1-2 (2000): 71-109.","category":"page"},{"location":"reference/value_iteration/#Value-iteration","page":"Value Iteration","title":"Value iteration","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"value_iteration","category":"page"},{"location":"reference/value_iteration/#IntervalMDP.value_iteration","page":"Value Iteration","title":"IntervalMDP.value_iteration","text":"value_iteration(problem::Problem{<:IntervalMarkovChain, <:Specification})\n\nSolve optimistic/pessimistic specification problems using value iteration for interval Markov chain.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\nmc = IntervalMarkovChain(prob, 1)\n\nterminal_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(terminal_states, time_horizon)\nspec = Specification(prop, Pessimistic)\nproblem = Problem(mc, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\nvalue_iteration(problem::Problem{<:IntervalMarkovDecisionProcess, <:Specification})\n\nSolve minimizes/mazimizes optimistic/pessimistic specification problems using value iteration for interval Markov decision processes. \n\nExamples\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_state = 1\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_state)\n\nterminal_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(terminal_states, time_horizon)\nspec = Specification(prop, Pessimistic, Maximize)\nproblem = Problem(mdp, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#Bellman-update","page":"Value Iteration","title":"Bellman update","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"bellman\nbellman!\nconstruct_ordering","category":"page"},{"location":"reference/value_iteration/#IntervalMDP.bellman","page":"Value Iteration","title":"IntervalMDP.bellman","text":"bellman(V, prob; max = true)\n\nCompute robust Bellman update with the value function V and the interval probabilities prob  that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the max keyword argument. That is, if max == true then an upper bound is computed and if max == false then a lower bound is computed.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nVprev = collect(1:15)\nVcur = bellman(Vprev, prob; max = true)\n\nnote: Note\nThis function will construct a workspace object for the ordering and an output vector. For a hot-loop, it is more efficient to use bellman! and pass in pre-allocated objects. See construct_ordering for how to pre-allocate the workspace.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.bellman!","page":"Value Iteration","title":"IntervalMDP.bellman!","text":"bellman!(ordering, Vres, V, prob; max = true)\n\nCompute in-place robust Bellman update with the value function V and the interval probabilities prob that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the max keyword argument. That is, if max == true then an upper bound is computed and if max == false then a lower bound is computed.\n\nThe output is constructed in the input Vres and returned. The ordering workspace object is also modified.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nV = collect(1:15)\nordering = construct_ordering(prob)\nVres = similar(V)\n\nVres = bellman!(ordering, Vres, V, prob; max = true)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IntervalMDP.construct_ordering","page":"Value Iteration","title":"IntervalMDP.construct_ordering","text":"construct_ordering(p::AbstractMatrix)\n\nConstruct a workspace for constructing and storing orderings of states, given a value function. If O-maximization is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nAn alternative constructor is construct_ordering(prob::IntervalProbabilities).\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To simplify the dicussion on the algorithmic choices, we will assume that the goal is to compute the maximizing pessimistic probability of reaching a set of states G, that is, ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"max_pi  min_eta  mathbbP_pieta leftomega in Omega  exists k in 0K  omega(k)in G  right","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"See Theory for more details on the theory behind IMDPs including strategies and adversaries; in this case the maximization and minimization operators respectively. The algorithms are easily adapted to other specifications, such as minimizing optimistic probability, which is useful for safety, or maximizing pessimitic discounted reward. Assume furthermore that the transition probabilities are represented as a sparse matrix. This is the most common representation for large models, and the algorithms are easily adapted to dense matrices with the sorting (see Sorting) being shared across states such that parallelizing this has a smaller impact on performance.","category":"page"},{"location":"algorithms/#Solving-reachability-as-value-iteration","page":"Algorithms","title":"Solving reachability as value iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Computing the solution to the above problem can be reframed in terms of value iteration. The value function V_k is the probability of reaching G in k steps or fewer. The value function is initialized to V_0(s) = 1 if s in G and V_0(s) = 0 otherwise. The value function is then iteratively updated according to the Bellman equation","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"beginaligned\n    V_0(s) = mathbf1_G(s) \n    V_k(s) = mathbf1_G(s) + mathbf1_Ssetminus G(s) max_a in A min_p_sain Gamma_sa sum_s in S V_k-1(s) p_sa(s)\nendaligned","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where mathbf1_G(s) = 1  if s in G and 0 otherwise is the indicator function for set G. This Bellman update is repeated until k = K, or if K = infty, the value function converges, i.e. V_k = V_k-1 for some k. The value function is then the solution to the problem. Exact convergence is virtually impossible to achieve in a finite number of iterations due to the finite precision of floating point numbers. Hence, we instead use a residual tolerance epsilon and stop when Bellman residual V_k - V_k-1 is less than the threshold, V_k - V_k-1_infty  epsilon.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In a more programmatic formulation, the algorithm (for K = infty) can be summarized as follows:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function value_iteration(system, spec)\n    V = initialize_value_function(spec)\n\n    while !converged(V)\n        V = bellman_update(V, system)\n    end\nend","category":"page"},{"location":"algorithms/#Efficient-value-iteration","page":"Algorithms","title":"Efficient value iteration","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Computing the Bellman update for can be done indepently for each state. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function bellman_update(V, system)\n    # Thread.@threads parallelize across available threads\n    Thread.@threads for s in states(system)\n        # Minimize over probability distributions in `Gamma_{s,a}`, i.e. pessimistic\n        V_state = minimize_feasible_dist(V, system, s)\n\n        # Maximize over actions\n        V[s] = maximum(V_state)\n    end\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"For each state, we need to compute the minimum over all feasible distributions per state-action pairs and the maximum over all actions for each state. The minimum over all feasible distributions can be computed as a solution to a Linear Programming (LP) problem, namely","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"    beginaligned\n        min_p_sa quad  sum_s in S V_k-1(s) cdot p_sa(s) \n        quad  underlineP(sas) leq p_sa(s) leq overlineP(sas) quad forall s in S \n        quad  sum_s in S p_sa(s) = 1 \n    endaligned","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"However, due to the particular structure of the LP problem, we can use a more efficient algorithm: O-maximization, or ordering-maximization [1]. In the case of pessimistic probability, we want to assign the most possible probability mass to the destinations with the smallest value of V_k-1, while obeying that the probability distribution is feasible, i.e. within the probability bounds and that it sums to 1. This is done by sorting the values of V_k-1 and then assigning state with the smallest value its upper bound, then the second smallest, and so on until the remaining mass must be assigned to the lower bound of the remaining states for probability distribution is feasible.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function minimize_feasible_dist(V, system, s)\n    # Sort values of `V` in ascending order\n    order = sortperm(V)\n\n    # Initialize distribution to lower bounds\n    p = lower_bounds(system, s)\n    rem = sum(p)\n\n    # Assign upper bounds to states with smallest values\n    # until remaining mass is zero\n    for idx in order\n        gap = upper_bounds(system, s)[idx] - p[idx]\n        if rem <= gap\n            p[idx] += rem\n            break\n        else\n            p[idx] += gap\n            rem -= gap\n        end\n    end\n\n    return p\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We abstract this algorithm into the sorting phase and the O-maximization phase: ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function minimize_feasible_dist(V, system, s)\n    # Sort values of `V` in ascending order\n    order = sortstates(V)\n    p = o_maximize(system, s, order)\n    return p\nend","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"When computing computing the above on a GPU, we can and should parallelize both the sorting and the O-maximization phase. In the following two sections, we will discuss how parallelize these phases.","category":"page"},{"location":"algorithms/#Sorting","page":"Algorithms","title":"Sorting","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Sorting in parallel on the GPU is a well-studied problem, and there are many algorithms for doing so. We choose to use bitonic sorting, which is a sorting network that is easily parallelized and implementable on a GPU. The idea is to merge bitonic subsets, i.e. sets with first increasing then decreasing subsets of equal size, of increasingly larger sizes and perform minor rounds of swaps to maintain the bitonic property. The figure below shows 3 major rounds to sort a set of 8 elements (each line represents an element, each arrow is a comparison pointing towards the larger element). The latency[1] of the sorting network is O((lg n)^2), and thus it scales well to larger number of elements. See Wikipedia for more details.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/#O-maximization","page":"Algorithms","title":"O-maximization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"In order to parallelize the O-maximization phase, observe that O-maximization implicity implements a cumulative sum according to the ordering over gaps and this is the only dependency between the states. Hence, if we can parallelize this cumulative sum, then we can parallelize the O-maximization phase. Luckily, there is a well-studied algorithm for computing the cumulative sum in parallel: tree reduction for prefix scan. The idea is best explained with figure below.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Here, we recursively compute the cumulative sum of larger and larger subsets of the array. The latency is O(lg n), and thus very efficient. See Wikipedia for more details. When implementing the tree reduction on GPU, it is possible to use warp shuffles to very efficiently perform tree reductions of up to 32 elements. For larger sets, shared memory to store the intermediate results, which is much faster than global memory. See CUDA Programming Model for more details on why these choices are important.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Putting it all together, we get the following (pseudo-code) algorithm for O-maximization:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"function o_maximize(system, s, order)\n    p = lower_bounds(system, s)\n    rem = 1 - sum(p)\n    gap = upper_bounds(system, s) - p\n\n    # Ordered cumulative sum of gaps\n    cumgap = cumulative_sum(gap[order])\n\n    @parallelize for (i, o) in enumerate(order)\n        rem_state = max(rem - cumgap[i] + gap[o], 0)\n        if gap[o] < rem_state\n            p[o] += gap[o]\n        else\n            p[o] += rem_state\n            break\n        end\n    end\n\n    return p\nend","category":"page"},{"location":"algorithms/#CUDA-Programming-Model","page":"Algorithms","title":"CUDA Programming Model","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"We here give a brief introduction to the CUDA programming model to understand to algorithmic choices. For a more in-depth introduction, see the CUDA C++ Programming Guide. The CUDA framework is Single-Instruction Multiple-Thread (SIMT) parallel execution platform and Application Programming Interface. This is in contrast to Single-Instruction Multiple-Data where all data must be processed homogeneously without control flow. SIMT makes CUDA more flexible for heterogeneous processing and control flow. The smallest execution unit in CUDA is a thread, which is a sequential processing of instructions. A thread is uniquely identified by its thread index, which allows indexing into the global data for parallel processing. A group of 32 threads[2] is called a warp, which will be executed mostly synchronously on a streaming multiprocessor. If control flow makes threads in a wrap diverge, instructions may need to be decoded twice and executed in two separate cycles. Due to this synchronous behavior, data can be shared in registers between threads in a warp for maximum performance. A collection of (up to) 1024 threads is called a block, and this is the largest aggregation that can be synchronized. Furthermore, threads in a block share the appropriately named shared memory. This is memory that is stored locally on the streaming multiprocessor for fast access. Note that shared memory is unintuitively faster than local memory (not to be confused with registers) due to local memory being allocated in device memory. Finally, a collection of (up to) 65536 blocks is called the grid of a kernel, which is the set of instructions to be executed. The grid is singular as only a single ever exists per launched kernel. Hence, if more blocks are necessary to process the amount of data, then a grid-strided loop or multiple kernels are necessary. ","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"(Image: )","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[1]: Note that when assessing parallel algorithms, the asymptotic performance is measured by the latency, which is the delay in the number of parallel operations, before the result is available. This is in contrast to traditional algorithms, which are assessed by the total number of operations.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"[2]: with consecutive thread indices aligned to a multiple of 32.","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The general procedure for using this package can be described in 3 steps","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Construct interval Markov process (IMC or IMDP)\nChoose specification (reachability or reach-avoid)\nCall value_iteration or satisfaction_prob.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"First, we construct a system. We can either construct an interval Markov chain (IMC) or an interval Markov decision process. (IMDP) Both systems consist of states, a designated initial state, and a transition matrix. In addition, an IMDP has actions.  An example of how to construct either is the following:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using IntervalMDP\n\n# IMC\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\ninitial_states = [1]  # Initial states are optional\nmc = IntervalMarkovChain(prob, initial_states)\n\n# IMDP\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_states = [1]  # Initial states are optional\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Note that for an IMDP, the transition probabilities are specified as a list of pairs from actions to transition probabilities for each state. The constructor will concatenate the transition probabilities into a single matrix, such that the columns represent source/action pairs and the rows represent target states. It will in addition construct a state pointer stateptr pointing to the first column of each state and concatenate a list of actions. See IntervalMarkovDecisionProcess for more details on how to construct an IMDP.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"For IMC, the transition probability structure is significantly simpler with source states on the columns and target states on the rows of the transition matrices.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next, we choose a specification. Currently, we support reachability, reach-avoid, and reward properties. For reachability, we specify a target set of states and for reach-avoid we specify a target set of states and an avoid set of states. Furthermore, we distinguish between finite and infinite horizon properties. In addition to the property, we need to specify whether we want to maximize or minimize the optimistic or pessimistic satistisfaction probability or discounted reward.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"## Properties\n# Reachability\ntarget_set = [3]\n\nprop = FiniteTimeReachability(target_set, 10)  # Time steps\nprop = InfiniteTimeReachability(target_set, 1e-6)  # Residual tolerance\n\n# Reach-avoid\ntarget_set = [3]\navoid_set = [2]\n\nprop = FiniteTimeReachAvoid(target_set, avoid_set, 10)  # Time steps\nprop = InfiniteTimeReachAvoid(target_set, avoid_set, 1e-6)  # Residual tolerance\n\n# Reward\nreward = [1.0, 2.0, 3.0]\ndiscount = 0.9  # Has to be between 0 and 1\n\nprop = FiniteTimeReward(reward, discount, 10)  # Time steps\nprop = InfiniteTimeReward(reward, discount, 1e-6)  # Residual tolerance\n\n## Specification\nspec = Specification(prop, Pessimistic, Maximize)\nspec = Specification(prop, Pessimistic, Minimize)\nspec = Specification(prop, Optimistic, Maximize)\nspec = Specification(prop, Optimistic, Minimize)\n\n## Combine system and specification in a Problem\nproblem = Problem(imdp_or_imc, spec)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Finally, we call value_iteration or satisfaction_prob to solve the specification. satisfaction_prob returns the probability of satisfying the specification from the initial condition, while value_iteration returns the value function for all states in addition to the number of iterations performed and the last Bellman residual.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"V, k, residual = value_iteration(problem)\nsat_prob = satisfaction_prob(problem)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"note: Note\nTo use multi-threading for parallelization, you need to either start julia with julia --threads <n|auto> where n is a positive integer or to set the environment variable JULIA_NUM_THREADS to the number of threads you want to use. For more information, see Multi-threading.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"tip: Tip\nFor less memory usage, it is recommended to use Sparse matrices and Int32 indices. ","category":"page"},{"location":"usage/#Sparse-matrices","page":"Usage","title":"Sparse matrices","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"A disadvantage of IMDPs is that the size of the transition matrices grows O(n^2 m) where n is the number of states and m is the number of actions. Quickly, this becomes infeasible to store in memory. However, IMDPs frequently have lots of sparsity we may exploit. We choose in particular to  store the transition matrices in the compressed sparse column (CSC) format. This is a format that is widely used in Julia and other languages, and is supported by many linear algebra operations. It consists of three arrays: colptr, rowval and nzval. The colptr array stores the indices of the first non-zero value in each column. The rowval array stores the row indices of the non-zero values, and the nzval array stores the non-zero values. We choose this format, since source states are on the columns (see IntervalProbabilities for more information about the structure of the transition probability matrices). Thus the non-zero values for each source state is stored in sequentially in memory, enabling efficient memory access.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use SparseMatrixCSC, we need to load SparseArrays. Below is an example of how to construct an IntervalMarkovChain with sparse transition matrices.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays\n\nlower = spzeros(3, 3)\nlower[2, 1] = 0.1\nlower[3, 1] = 0.2\nlower[1, 2] = 0.5\nlower[2, 2] = 0.3\nlower[3, 2] = 0.1\nlower[3, 3] = 1.0\n\nlower","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"upper = spzeros(3, 3)\nupper[1, 1] = 0.5\nupper[2, 1] = 0.6\nupper[3, 1] = 0.7\nupper[1, 2] = 0.7\nupper[2, 2] = 0.5\nupper[3, 2] = 0.3\nupper[3, 3] = 1.0\n\nupper","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"prob = IntervalProbabilities(; lower = lower, upper = upper)\ninitial_state = 1\nmc = IntervalMarkovChain(prob, initial_state)\n","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If you know that the matrix can be built sequentially, you can use the SparseMatrixCSC constructor directly with colptr, rowval and nzval. This is more efficient, since setindex! of SparseMatrixCSC needs to perform a binary search to find the correct index to insert the value, and possibly expand the size of the array.","category":"page"},{"location":"usage/#CUDA","page":"Usage","title":"CUDA","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Part of the innovation of this package is GPU-accelerated value iteration via CUDA. This includes not only trivial parallelization across states but also parallel algorithms for O-maximization within each state for better computational efficiency and coalesced memory access for more speed. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to first install CUDA.jl. For more information about this, see Installation. Next, you need to load the package with the following command:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using CUDA","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Loading CUDA will automatically load an extension that defines value iteration with CUDA arrays. It has been separated out into an extension to reduce precompilation time for users that do not need CUDA. Note that loading CUDA on a system without a CUDA-capable GPU, will not cause any errors, but will simply not load the extension. You can check if CUDA is correctly loaded using CUDA.is_functional().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to transfer the model to the GPU. Once on the GPU, you can use the same functions as the CPU implementation. Using Julia's multiple dispatch, the package will automatically call the appropriate functions for the given variable types.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Similar to CUDA.jl, we provide a cu function that transfers the model to the GPU[1]. You can either transfer the entire model or transfer the transition matrices separately. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# Transfer entire model to GPU\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    ),\n)\n\nmc = IntervalMDP.cu(IntervalMarkovChain(prob, 1))\n\n# Transfer transition matrices separately\nprob = IntervalProbabilities(;\n    lower = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    )),\n    upper = IntervalMDP.cu(sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    )),\n)\n\nmc = IntervalMarkovChain(prob,[1])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"[1]: The difference to CUDA.jls cu function is that we allow the specification of both the value and index type, which is important due to register pressure. To reduce register pressure but maintain accuracy, we are opinoinated to Float64 values and Int32 indices.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = IntervalMDP","category":"page"},{"location":"#IntervalMDP","page":"Home","title":"IntervalMDP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"IntervalMDP.jl is a Julia package for modeling and certifying Interval Markov Decision Processes (IMDPs) via Value Iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"IMDPs are a generalization of Markov Decision Processes (MDPs) where the transition probabilities are represented by intervals instead of point values, to model uncertainty. IMDPs are also frequently chosen as the model for abstracting the dynamics of a stochastic system, as one may compute upper and lower bounds on transitioning from one region to another.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The aim of this package is to provide a user-friendly interface to solve value iteration for IMDPs with great efficiency. Furthermore, it provides methods for accelerating the computation of the certificate using CUDA hardware. See Algorithms for algorithmic advances that this package introduces for enabling better use of the available hardware and higher performance.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"O-maximization and value iteration\nDense and sparse matrix support\nParametric probability types for customizable precision\nMultithreaded CPU and CUDA-accelerated value iteration\nData loading and writing in formats by various tools (PRISM, bmdp-tool, IntervalMDP.jl)","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nUntil now, all state-of-the-art tools for IMDPs have been standalone programs.  This is explicitly a package, enabling better integration with other tools and libraries.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package requires Julia v1.9 or later. Refer to the official documentation on how to install it for your system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install IntervalMDP.jl, use the following command inside Julia's REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"IntervalMDP\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to use the CUDA extension, you also need to install CUDA.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"CUDA\")","category":"page"}]
}
