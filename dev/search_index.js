var documenterSearchIndex = {"docs":
[{"location":"reference/certification/#Certification","page":"Certification & synthesis","title":"Certification","text":"","category":"section"},{"location":"reference/certification/","page":"Certification & synthesis","title":"Certification & synthesis","text":"satisfaction_probability\ncontrol_synthesis","category":"page"},{"location":"reference/certification/#IMDP.satisfaction_probability","page":"Certification & synthesis","title":"IMDP.satisfaction_probability","text":"satisfaction_probability(problem::Problem{<:IntervalMarkovProcess, <:AbstractReachability})\n\nCompute the probability of satisfying the reachability-like specification from the initial state.\n\n\n\n\n\n","category":"function"},{"location":"reference/certification/#IMDP.control_synthesis","page":"Certification & synthesis","title":"IMDP.control_synthesis","text":"control_synthesis(problem::Problem{<:IntervalMarkovDecisionProcess})\n\nCompute the optimal control policy for the given problem (system + specification). If the specification is finite time, then the policy is time-varying, with the returned policy being in step order (i.e., the first element of the returned vector is the policy for the first time step). If the specification is infinite time, then the policy is stationary and only a single vector of length num_states(system) is returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#Problem","page":"Specifications","title":"Problem","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"Problem\nsystem\nspecification\nSpecification\nsystem_property\nProperty\nsatisfaction_mode\nSatisfactionMode\nstrategy_mode\nStrategyMode","category":"page"},{"location":"reference/specifications/#IMDP.Problem","page":"Specifications","title":"IMDP.Problem","text":"Problem{S <: IntervalMarkovProcess, F <: Specification}\n\nA problem is a tuple of an interval Markov process and a specification.\n\nFields\n\nsystem::S: interval Markov process.\nspec::F: specification (either temporal logic or reachability-like).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.system","page":"Specifications","title":"IMDP.system","text":"system(prob::Problem)\n\nReturn the system of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IMDP.specification","page":"Specifications","title":"IMDP.specification","text":"specification(prob::Problem)\n\nReturn the specification of a problem.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IMDP.Specification","page":"Specifications","title":"IMDP.Specification","text":"Specification{F <: Property}\n\nA specfication is a property together with a satisfaction mode and a strategy mode.  The satisfaction mode is either Optimistic or Pessimistic. See SatisfactionMode for more details. The strategy  mode is either Maxmize or Minimize. See StrategyMode for more details.\n\nFields\n\nprop::F: verification property (either temporal logic or reachability-like).\nsatisfaction::SatisfactionMode: satisfaction mode (either optimistic or pessimistic). Default is pessimistic.\nstrategy::StrategyMode: strategy mode (either maximize or minimize). Default is maximize.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.system_property","page":"Specifications","title":"IMDP.system_property","text":"system_property(spec::Specification)\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IMDP.Property","page":"Specifications","title":"IMDP.Property","text":"Property\n\nSuper type for all system Property\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.satisfaction_mode","page":"Specifications","title":"IMDP.satisfaction_mode","text":"satisfaction_mode(spec::Specification)\n\nReturn the satisfaction mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IMDP.SatisfactionMode","page":"Specifications","title":"IMDP.SatisfactionMode","text":"SatisfactionMode\n\nWhen computing the satisfaction probability of a property over an interval Markov process, be it IMC or IMDP, the desired satisfaction probability to verify can either be Optimistic or Pessimistic. That is, upper and lower bounds on the satisfaction probability within the probability uncertainty.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.strategy_mode","page":"Specifications","title":"IMDP.strategy_mode","text":"strategy_mode(spec::Specification)\n\nReturn the strategy mode of a specification.\n\n\n\n\n\n","category":"function"},{"location":"reference/specifications/#IMDP.StrategyMode","page":"Specifications","title":"IMDP.StrategyMode","text":"StrategyMode\n\nWhen computing the satisfaction probability of a property over an IMDP, the strategy can either maximize or minimize the satisfaction probability (wrt. the satisfaction mode).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Temporal-logic","page":"Specifications","title":"Temporal logic","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"IMDP.AbstractTemporalLogic\n\nLTLFormula\nIMDP.isfinitetime(spec::LTLFormula)\nLTLfFormula\nIMDP.isfinitetime(spec::LTLfFormula)\ntime_horizon(spec::LTLfFormula)\nPCTLFormula","category":"page"},{"location":"reference/specifications/#IMDP.AbstractTemporalLogic","page":"Specifications","title":"IMDP.AbstractTemporalLogic","text":"AbstractTemporalLogic\n\nSuper type for temporal logic property\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.LTLFormula","page":"Specifications","title":"IMDP.LTLFormula","text":"LTLFormula\n\nLinear Temporal Logic (LTL) property (first-order logic + next and until operators) [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] Vardi, M.Y. (1996). An automata-theoretic approach to linear temporal logic. In: Moller, F., Birtwistle, G. (eds) Logics for Concurrency. Lecture Notes in Computer Science, vol 1043. Springer, Berlin, Heidelberg.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{LTLFormula}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::LTLFormula)\n\nReturn false for an LTL formula. LTL formulas are not finite time property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.LTLfFormula","page":"Specifications","title":"IMDP.LTLfFormula","text":"LTLfFormula\n\nAn LTL formula over finite traces [1]. See LTLFormula for the structure of LTL formulas. Let ϕ denote the formula, M denote an interval Markov process, and H the time horizon. Then compute M  ϕ within traces of length H.\n\nFields\n\nformula::String: LTL formula\ntime_horizon::T: Time horizon of the finite traces \n\n[1] Giuseppe De Giacomo and Moshe Y. Vardi. 2013. Linear temporal logic and linear dynamic logic on finite traces. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (IJCAI '13). AAAI Press, 854–860.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{LTLfFormula}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(spec::LTLfFormula)\n\nReturn true for an LTLf formula. LTLf formulas are specifically over finite traces.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.time_horizon-Tuple{LTLfFormula}","page":"Specifications","title":"IMDP.time_horizon","text":"time_horizon(spec::LTLfFormula)\n\nReturn the time horizon of an LTLf formula.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.PCTLFormula","page":"Specifications","title":"IMDP.PCTLFormula","text":"PCTLFormula\n\nA Probabilistic Computation Tree Logic (PCTL) formula [1]. Let ϕ denote the formula and M denote an interval Markov process. Then compute M  ϕ.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#Reachability","page":"Specifications","title":"Reachability","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachability\n\nFiniteTimeReachability\nIMDP.isfinitetime(spec::FiniteTimeReachability)\nterminal_states(spec::FiniteTimeReachability)\nreach(spec::FiniteTimeReachability)\ntime_horizon(spec::FiniteTimeReachability)\n\nInfiniteTimeReachability\nIMDP.isfinitetime(spec::InfiniteTimeReachability)\nterminal_states(spec::InfiniteTimeReachability)\nreach(spec::InfiniteTimeReachability)\nconvergence_eps(spec::InfiniteTimeReachability)","category":"page"},{"location":"reference/specifications/#IMDP.AbstractReachability","page":"Specifications","title":"IMDP.AbstractReachability","text":"AbstractReachability\n\nSuper type for all reachability-like property.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.FiniteTimeReachability","page":"Specifications","title":"IMDP.FiniteTimeReachability","text":"FiniteTimeReachability{T <: Integer, VT <: AbstractVector{T}}\n\nFinite time reachability specified by a set of target/terminal states and a time horizon.  That is, if T is the set of target states and H is the time horizon, compute ℙ(k = 0H s_k  T).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachability)\n\nReturn true for FiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.terminal_states-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IMDP.terminal_states","text":"terminal_states(spec::FiniteTimeReachability)\n\nReturn the set of terminal states of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reach-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IMDP.reach","text":"terminal_states(prop::FiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a finite time reachability prop. This is equivalent for terminal_states(prop::FiniteTimeReachability) for a regular reachability property. See FiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.time_horizon-Tuple{FiniteTimeReachability}","page":"Specifications","title":"IMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachability)\n\nReturn the time horizon of a finite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.InfiniteTimeReachability","page":"Specifications","title":"IMDP.InfiniteTimeReachability","text":"InfiniteTimeReachability{R <: Real, T <: Integer, VT <: AbstractVector{T}}\n\nInfiniteTimeReachability is similar to FiniteTimeReachability except that the time horizon is infinite. The convergence threshold is that the largest value of the most recent Bellman residual is less than eps.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachability)\n\nReturn false for InfiniteTimeReachability.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.terminal_states-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachability)\n\nReturn the set of terminal states of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reach-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IMDP.reach","text":"reach(prop::InfiniteTimeReachability)\n\nReturn the set of states with which to compute reachbility for a infinite time reachability property. This is equivalent for terminal_states(prop::InfiniteTimeReachability) for a regular reachability property. See InfiniteTimeReachAvoid for a more complex property where the reachability and terminal states differ.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.convergence_eps-Tuple{InfiniteTimeReachability}","page":"Specifications","title":"IMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachability)\n\nReturn the convergence threshold of an infinite time reachability property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reach-avoid","page":"Specifications","title":"Reach-avoid","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReachAvoid\n\nFiniteTimeReachAvoid\nIMDP.isfinitetime(spec::FiniteTimeReachAvoid)\nterminal_states(spec::FiniteTimeReachAvoid)\nreach(spec::FiniteTimeReachAvoid)\navoid(spec::FiniteTimeReachAvoid)\ntime_horizon(spec::FiniteTimeReachAvoid)\n\nInfiniteTimeReachAvoid\nIMDP.isfinitetime(spec::InfiniteTimeReachAvoid)\nterminal_states(spec::InfiniteTimeReachAvoid)\nreach(spec::InfiniteTimeReachAvoid)\navoid(spec::InfiniteTimeReachAvoid)\nconvergence_eps(spec::InfiniteTimeReachAvoid)","category":"page"},{"location":"reference/specifications/#IMDP.AbstractReachAvoid","page":"Specifications","title":"IMDP.AbstractReachAvoid","text":"AbstractReachAvoid\n\nA property of reachability that includes a set of states to avoid.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.FiniteTimeReachAvoid","page":"Specifications","title":"IMDP.FiniteTimeReachAvoid","text":"FiniteTimeReachAvoid{T <: Integer, VT <: AbstractVector{T}}\n\nFinite time reach-avoid specified by a set of target/terminal states, a set of avoid states, and a time horizon. That is, if T is the set of target states, A is the set of states to avoid, and H is the time horizon, compute ℙ(k = 0H s_k  T and k = 0k s_k  A).\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReachAvoid)\n\nReturn true for FiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.terminal_states-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.terminal_states","text":"terminal_states(prop::FiniteTimeReachAvoid)\n\nReturn the set of terminal states of a finite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reach-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.reach","text":"reach(prop::FiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.avoid-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.avoid","text":"avoid(prop::FiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.time_horizon-Tuple{FiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReachAvoid)\n\nReturn the time horizon of a finite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.InfiniteTimeReachAvoid","page":"Specifications","title":"IMDP.InfiniteTimeReachAvoid","text":"InfiniteTimeReachAvoid{R <: Real, T <: Integer, VT <: AbstractVector{T}}\n\nInfiniteTimeReachAvoid is similar to FiniteTimeReachAvoid except that the time horizon is infinite.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReachAvoid)\n\nReturn false for InfiniteTimeReachAvoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.terminal_states-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.terminal_states","text":"terminal_states(prop::InfiniteTimeReachAvoid)\n\nReturn the set of terminal states of an infinite time reach-avoid property. That is, the union of the reach and avoid sets.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reach-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.reach","text":"reach(prop::InfiniteTimeReachAvoid)\n\nReturn the set of target states.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.avoid-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.avoid","text":"avoid(prop::InfiniteTimeReachAvoid)\n\nReturn the set of states to avoid.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.convergence_eps-Tuple{InfiniteTimeReachAvoid}","page":"Specifications","title":"IMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReachAvoid)\n\nReturn the convergence threshold of an infinite time reach-avoid property.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#Reward-specification","page":"Specifications","title":"Reward specification","text":"","category":"section"},{"location":"reference/specifications/","page":"Specifications","title":"Specifications","text":"AbstractReward\n\nFiniteTimeReward\nIMDP.isfinitetime(spec::FiniteTimeReward)\nreward(spec::FiniteTimeReward)\ndiscount(spec::FiniteTimeReward)\ntime_horizon(spec::FiniteTimeReward)\n\nInfiniteTimeReward\nIMDP.isfinitetime(spec::InfiniteTimeReward)\nreward(spec::InfiniteTimeReward)\ndiscount(spec::InfiniteTimeReward)\nconvergence_eps(spec::InfiniteTimeReward)","category":"page"},{"location":"reference/specifications/#IMDP.AbstractReward","page":"Specifications","title":"IMDP.AbstractReward","text":"AbstractReward{R <: Real}\n\nSuper type for all reward specifications.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.FiniteTimeReward","page":"Specifications","title":"IMDP.FiniteTimeReward","text":"FiniteTimeReward{R <: Real, T <: Integer, VR <: AbstractVector{R}}\n\nFiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor. The time horizon is finite, so the discount factor is optional and  the optimal policy will be time-varying.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{FiniteTimeReward}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::FiniteTimeReward)\n\nReturn true for FiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reward-Tuple{FiniteTimeReward}","page":"Specifications","title":"IMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.discount-Tuple{FiniteTimeReward}","page":"Specifications","title":"IMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.time_horizon-Tuple{FiniteTimeReward}","page":"Specifications","title":"IMDP.time_horizon","text":"time_horizon(prop::FiniteTimeReward)\n\nReturn the time horizon of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.InfiniteTimeReward","page":"Specifications","title":"IMDP.InfiniteTimeReward","text":"InfiniteTimeReward{R <: Real, VR <: AbstractVector{R}}\n\nInfiniteTimeReward is a property of rewards assigned to each state at each iteration and a discount factor for guaranteed convergence. The time horizon is infinite, so the optimal policy will be stationary.\n\n\n\n\n\n","category":"type"},{"location":"reference/specifications/#IMDP.isfinitetime-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IMDP.isfinitetime","text":"isfinitetime(prop::InfiniteTimeReward)\n\nReturn false for InfiniteTimeReward.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.reward-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IMDP.reward","text":"reward(prop::FiniteTimeReward)\n\nReturn the reward vector of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.discount-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IMDP.discount","text":"discount(prop::FiniteTimeReward)\n\nReturn the discount factor of a finite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"reference/specifications/#IMDP.convergence_eps-Tuple{InfiniteTimeReward}","page":"Specifications","title":"IMDP.convergence_eps","text":"convergence_eps(prop::InfiniteTimeReward)\n\nReturn the convergence threshold of an infinite time reward optimization.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"Index","title":"Index","text":"","category":"page"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The general procedure for using this package can be described in 3 steps","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Construct interval Markov process (IMC or IMDP)\nChoose specification (reachability or reach-avoid)\nCall value_iteration or satisfaction_prob.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"First, we construct a system. We can either construct an interval Markov chain (IMC) or an interval Markov decision process. (IMDP) Both systems consist of states, a designated initial state, and a transition matrix. In addition, an IMDP has actions.  An example of how to construct either is the following:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# IMC\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\ninitial_states = [1]  # Initial states are optional\nmc = IntervalMarkovChain(prob, initial_states)\n\n# IMDP\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_states = [1]  # Initial states are optional\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Note that for an IMDP, the transition probabilities are specified as a list of pairs from actions to transition probabilities for each state. The constructor with concatenate the transition probabilities into a single matrix, such that the columns represent source/action pairs and the rows represent target states. It will in addition construct a state pointer stateptr pointing to the first column of each state and concatenate a list of actions. See IntervalMarkovDecisionProcess for more details on how to construct an IMDP.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"For IMC, it is signifianctly simpler with source states on the columns and target states on the rows of the transition matrices.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next, we choose a specification. Currently, we support reachability, reach-avoid, and reward specifications. For reachability, we specify a target set of states and for reach-avoid we specify a target set of states and an avoid set of states. Furthermore, we distinguish between finite and infinite horizon specifications.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"## Properties\n# Reachability\ntarget_set = [3]\n\nprop = FiniteHorizonReachability(target_set, 10)  # Time steps\nprop = InfiniteHorizonReachability(target_set, 1e-6)  # Residual tolerance\n\n# Reach-avoid\ntarget_set = [3]\navoid_set = [2]\n\nprop = FiniteHorizonReachAvoid(target_set, avoid_set, 10)  # Time steps\nprop = InfiniteHorizonReachAvoid(target_set, avoid_set, 1e-6)  # Residual tolerance\n\n# Reward\nreward = [1.0, 2.0, 3.0]\ndiscount = 0.9  # Has to be between 0 and 1\n\nprop = FiniteHorizonReward(reward, discount, 10)  # Time steps\nprop = InfiniteHorizonReward(reward, discount, 1e-6)  # Residual tolerance\n\n## Specification\nspec = Specification(prop, Pessimistic, Maximize)\nspec = Specification(prop, Pessimistic, Minimize)\nspec = Specification(prop, Optimistic, Maximize)\nspec = Specification(prop, Optimistic, Minimize)\n\n## Combine system and specification in a Problem\nproblem = Problem(imdp_or_imc, spec)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Finally, we call value_iteration or satisfaction_prob to solve the specification. satisfaction_prob returns the probability of satisfying the specification from the initial condition, while value_iteration returns the value function for all states in addition to the number of iterations performed and the last Bellman residual.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"V, k, residual = value_iteration(problem)\nsat_prob = satisfaction_prob(problem)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"tip: Tip\nFor less memory usage, it is recommended to use Sparse matrices and Int32 indices. ","category":"page"},{"location":"usage/#Sparse-matrices","page":"Usage","title":"Sparse matrices","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"A disadvantage of IMDPs is that the size of the transition matrices grows O(n^2 m) where n is the number of states and m is the number of actions. Quickly, this becomes infeasible to store in memory. However, IMDPs frequently have lots of sparsity we may exploit. We choose in particular to  store the transition matrices in the compressed sparse column (CSC) format. This is a format that is widely used in Julia and other languages, and is supported by many linear algebra operations. It consists of three arrays: colptr, rowval and nzval. The colptr array stores the indices of the first non-zero value in each column. The rowval array stores the row indices of the non-zero values, and the nzval array stores the non-zero values. We choose this format, since source states are on the columns (see IntervalProbabilities for more information about the structure of the transition probability matrices). Thus the non-zero values for each source state is stored in sequentially in memory, enabling efficient memory access.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use SparseMatrixCSC, we need to load SparseArrays. Below is an example of how to construct an IntervalMarkovChain with sparse transition matrices.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays\n\nlower = spzeros(3, 3)\nlower[2, 1] = 0.1\nlower[3, 1] = 0.2\nlower[1, 2] = 0.5\nlower[2, 2] = 0.3\nlower[3, 2] = 0.1\nlower[3, 3] = 1.0\n\nlower","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SparseArrays","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"upper = spzeros(3, 3)\nupper[1, 1] = 0.5\nupper[2, 1] = 0.6\nupper[3, 1] = 0.7\nupper[1, 2] = 0.7\nupper[2, 2] = 0.5\nupper[3, 2] = 0.3\nupper[3, 3] = 1.0\n\nupper","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"prob = IntervalProbabilities(; lower = lower, upper = upper)\ninitial_state = 1\nmc = IntervalMarkovChain(prob, initial_state)\n","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If you know that the matrix can be built sequentially, you can use the SparseMatrixCSC constructor directly with colptr, rowval and nzval. This is more efficient, since setindex! of SparseMatrixCSC needs to perform a binary search to find the correct index to insert the value, and possibly expand the size of the array.","category":"page"},{"location":"usage/#CUDA","page":"Usage","title":"CUDA","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Part of the innovation of this package is GPU-accelerated value iteration via CUDA. This includes not only trivial parallelization across states but also parallel algorithms for O-maximization within each state for better computational efficiency and coalesced memory access for more speed. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to first install CUDA.jl. For more information about this, see Installation. Next, you need to load the package with the following command:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using CUDA","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Loading CUDA will automatically load an extension that defines value iteration with CUDA arrays. It has been separated out into an extension to reduce precompilation time for users that do not need CUDA. Note that loading CUDA on a system without a CUDA-capable GPU, will not cause any errors, but will simply not load the extension. You can check if CUDA is correctly loaded using CUDA.is_functional().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"To use CUDA, you need to transfer the model to the GPU. Once on the GPU, you can use the same functions as the CPU implementation. Using Julia's multiple dispatch, the package will automatically call the appropriate functions for the given variable types.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Similar to CUDA.jl, we provide a cu function that transfers the model to the GPU[1]. You can either transfer the entire model or transfer the transition matrices separately. ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"# Transfer entire model to GPU\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    ),\n)\n\nmc = IMDP.cu(IntervalMarkovChain(prob, 1))\n\n# Transfer transition matrices separately\nprob = IntervalProbabilities(;\n    lower = IMDP.cu(sparse_hcat(\n        SparseVector(3, [2, 3], [0.1, 0.2]),\n        SparseVector(3, [1, 2, 3], [0.5, 0.3, 0.1]),\n        SparseVector(3, [3], [1.0]),\n    )),\n    upper = IMDP.cu(sparse_hcat(\n        SparseVector(3, [1, 2, 3], [0.5, 0.6, 0.7]),\n        SparseVector(3, [1, 2, 3], [0.7, 0.5, 0.3]),\n        SparseVector(3, [3], [1.0]),\n    )),\n)\n\nmc = IntervalMarkovChain(prob,[1])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"[1]: The difference to CUDA.jls cu function is that we allow the specification of both the value and index type, which is important due to register pressure. To reduce register pressure but maintain accuracy, we are opinoinated to Float64 values and Int32 indices.","category":"page"},{"location":"reference/data/#Data-formats","page":"Data Storage","title":"Data formats","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"CurrentModule = IMDP.Data","category":"page"},{"location":"reference/data/#PRISM","page":"Data Storage","title":"PRISM","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"write_prism_file\nread_prism_file","category":"page"},{"location":"reference/data/#IMDP.Data.write_prism_file","page":"Data Storage","title":"IMDP.Data.write_prism_file","text":"write_prism_file(path_without_file_ending, problem)\n\nWrite the files required by PRISM explicit engine/format to \n\npath_without_file_ending.sta (states),\npath_without_file_ending.lab (labels),\npath_without_file_ending.tra (transitions), and\npath_without_file_ending.pctl (properties).\n\nIf the specification is a reward optimization problem, then a state rewards file .srew is also written.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.read_prism_file","page":"Data Storage","title":"IMDP.Data.read_prism_file","text":"read_prism_file(path_without_file_ending)\n\nRead PRISM explicit file formats and pctl file, and return a Problem including system and specification.\n\nSee PRISM Explicit Model Files for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#bmdp-tool","page":"Data Storage","title":"bmdp-tool","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_bmdp_tool_file\nwrite_bmdp_tool_file","category":"page"},{"location":"reference/data/#IMDP.Data.read_bmdp_tool_file","page":"Data Storage","title":"IMDP.Data.read_bmdp_tool_file","text":"read_bmdp_tool_file(path)\n\nRead a bmdp-tool transition probability file and return an IntervalMarkovDecisionProcess and a list of terminal states. From the file format, it is not clear if the desired reachability verification if the reachability specification is finite or infinite horizon, the satisfaction_mode is pessimistic or optimistic, or if the actions should minimize or maximize the probability of reachability.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.write_bmdp_tool_file","page":"Data Storage","title":"IMDP.Data.write_bmdp_tool_file","text":"write_bmdp_tool_file(path, problem::Problem)\n\nWrite a bmdp-tool transition probability file for the given an IMDP and a reachability specification. The file will not contain enough information to specify a reachability specification. The remaining parameters are rather command line arguments.\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovDecisionProcess, spec::AbstractReachability)\n\n\n\n\n\nwrite_bmdp_tool_file(path, mdp::IntervalMarkovDecisionProcess, terminal_states::Vector{T})\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.jl","page":"Data Storage","title":"IMDP.jl","text":"","category":"section"},{"location":"reference/data/","page":"Data Storage","title":"Data Storage","text":"read_imdp_jl\nread_imdp_jl_model\nread_imdp_jl_spec\nwrite_imdp_jl_model\nwrite_imdp_jl_spec","category":"page"},{"location":"reference/data/#IMDP.Data.read_imdp_jl","page":"Data Storage","title":"IMDP.Data.read_imdp_jl","text":"read_imdp_jl_file(path)\n\nRead an IMDP.jl data file and return an IntervalMarkovDecisionProcess or IntervalMarkovChain and a list of terminal states. \n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.read_imdp_jl_model","page":"Data Storage","title":"IMDP.Data.read_imdp_jl_model","text":"read_imdp_jl_model(model_path)\n\nRead an IntervalMarkovDecisionProcess or IntervalMarkovChain from an IMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.read_imdp_jl_spec","page":"Data Storage","title":"IMDP.Data.read_imdp_jl_spec","text":"read_imdp_jl_spec(spec_path)\n\nRead a Specification from an IMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.write_imdp_jl_model","page":"Data Storage","title":"IMDP.Data.write_imdp_jl_model","text":"write_imdp_jl_model(model_path, mdp_or_mc)\n\nWrite an IntervalMarkovDecisionProcess or IntervalMarkovChain to an IMDP.jl system file (netCDF sparse format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/data/#IMDP.Data.write_imdp_jl_spec","page":"Data Storage","title":"IMDP.Data.write_imdp_jl_spec","text":"write_imdp_jl_spec(spec_path, spec::Specification)\n\nWrite a Specification to an IMDP.jl spec file (JSON-format).\n\nSee Data storage formats for more information on the file format.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#Value-iteration","page":"Value Iteration","title":"Value iteration","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"value_iteration","category":"page"},{"location":"reference/value_iteration/#IMDP.value_iteration","page":"Value Iteration","title":"IMDP.value_iteration","text":"value_iteration(problem::Problem{<:IntervalMarkovChain, Specification{<:AbstractReachability}})\n\nSolve optimistic/pessimistic reachability and reach-avoid problems using value iteration for interval Markov chain.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\nmc = IntervalMarkovChain(prob, 1)\n\nterminal_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(terminal_states, time_horizon)\nspec = Specification(prop, Pessimistic)\nproblem = Problem(mc, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\nvalue_iteration(problem::Problem{<:IntervalMarkovChain, Specification{<:AbstractReward}}\n)\n\nFind pessimistic/optimistic reward using value iteration for interval Markov chain.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\nmc = IntervalMarkovChain(prob, 1)\n\nreward = [2.0, 1.0, 0.0]\ndiscount = 0.9\ntime_horizon = 10\nprop = FiniteTimeReward(reward, discount, time_horizon)\nspec = Specification(prop, Pessimistic)\nproblem = Problem(mc, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\nvalue_iteration(problem::Problem{<:IntervalMarkovDecisionProcess, Specification{<:AbstractReachability}})\n\nSolve minimizes/mazimizes optimistic/pessimistic reachability and reach-avoid problems using value iteration for interval Markov decision processes. \n\nExamples\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_state = 1\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_state)\n\nterminal_states = [3]\ntime_horizon = 10\nprop = FiniteTimeReachability(terminal_states, time_horizon)\nspec = Specification(prop, Pessimistic, Maximize)\nproblem = Problem(mdp, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\nvalue_iteration(problem::Problem{<:IntervalMarkovDecisionProcess, Specification{<:AbstractReward}})\n\nFind optimistic/pessimistic reward using value iteration for interval Markov decision process.\n\nExamples\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_state = 1\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_state)\n\nreward = [2.0, 1.0, 0.0]\ndiscount = 0.9\ntime_horizon = 10\nprop = FiniteTimeReward(reward, discount, time_horizon)\nspec = Specification(prop, Pessimistic, Maximize)\nproblem = Problem(mdp, spec)\nV, k, residual = value_iteration(problem)\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#O-maximization","page":"Value Iteration","title":"O-maximization","text":"","category":"section"},{"location":"reference/value_iteration/","page":"Value Iteration","title":"Value Iteration","text":"ominmax\nominmax!\npartial_ominmax\npartial_ominmax!\nconstruct_ordering","category":"page"},{"location":"reference/value_iteration/#IMDP.ominmax","page":"Value Iteration","title":"IMDP.ominmax","text":"ominmax(prob, V; max = true)\n\nCompute probability assignment within the interval probabilities prob that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the max keyword argument. That is, if max == true then an upper bound is computed and if max == false then a lower bound is computed.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nV = collect(1:15)\np = ominmax(prob, V; max = true)\n\nnote: Note\nThis function will construct a workspace object for the ordering and an output vector. For a hot-loop, it is more efficient to use ominmax! and pass in pre-allocated objects. See construct_ordering for how to pre-allocate the workspace.\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IMDP.ominmax!","page":"Value Iteration","title":"IMDP.ominmax!","text":"ominmax!(ordering, p, prob, V; max = true)\n\nCompute in-place the probability assignment within the interval probabilities prob that upper or lower bounds the expectation of the value function V via O-maximization [1]. Whether the expectation is maximized or minimized is determined by the max keyword argument. That is, if max == true then an upper bound is computed and if max == false then a lower bound is computed.\n\nThe output is constructed in the input vector p and returned. The ordering workspace object is also modified.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\nV = collect(1:15)\nordering = construct_ordering(prob)\np = deepcopy(gap(p))\n\np = ominmax!(ordering, p, prob, V; max = true)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IMDP.partial_ominmax","page":"Value Iteration","title":"IMDP.partial_ominmax","text":"partial_ominmax(prob, V, indices; max = true)\n\nPerform O-maximization on a subset of source states or source/action pairs according to indices. This corresponds to the columns in prob. See ominmax for more details on what O-maximization is.\n\nnote: Note\nThis function will construct a workspace object for the ordering and an output vector. For a hot-loop, it is more efficient to use ominmax! and pass in pre-allocated objects. See construct_ordering for how to pre-allocate the workspace.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IMDP.partial_ominmax!","page":"Value Iteration","title":"IMDP.partial_ominmax!","text":"partial_ominmax!(ordering, p, prob, V, indices; max = true)\n\nPerform O-maximization in-place on a subset of source states or source/action pairs according to indices. This corresponds to the columns in prob. See ominmax for more details on what O-maximization is.\n\n\n\n\n\n","category":"function"},{"location":"reference/value_iteration/#IMDP.construct_ordering","page":"Value Iteration","title":"IMDP.construct_ordering","text":"construct_ordering(p::AbstractMatrix)\n\nConstruct a workspace for constructing and storing orderings of states, given a value function. If O-maximization is used in a hot-loop, it is more efficient to use this function to preallocate the workspace and reuse across iterations.\n\nAn alternative constructor is construct_ordering(prob::IntervalProbabilities).\n\n\n\n\n\n","category":"function"},{"location":"data/#Data-storage-formats","page":"Data formats","title":"Data storage formats","text":"","category":"section"},{"location":"data/","page":"Data formats","title":"Data formats","text":"ASCII seems to dominate (but is very inefficient)\nBinary formats are more efficient but not standardized","category":"page"},{"location":"data/#PRISM","page":"Data formats","title":"PRISM","text":"","category":"section"},{"location":"data/#bmdp-tool","page":"Data formats","title":"bmdp-tool","text":"","category":"section"},{"location":"data/#IMDP.jl","page":"Data formats","title":"IMDP.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = IMDP","category":"page"},{"location":"#IMDP","page":"Home","title":"IMDP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"IMDP is a Julia package for modeling and certifying Interval Markov Decision Processes (IMDPs) via Value Iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"IMDPs are a generalization of Markov Decision Processes (MDPs) where the transition probabilities are represented by intervals instead of point values, to model uncertainty. IMDPs are also frequently chosen as the model for abstracting the dynamics of a stochastic system, as one may compute upper and lower bounds on transitioning from one region to another.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The aim of this package is to provide a user-friendly interface to solve value iteration for IMDPs with great efficiency. Furthermore, it provides methods for accelerating the computation of the certificate using CUDA hardware. See Algorithms for algorithmic advances that this package introduces for enabling better use of the available hardware and higher performance.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"O-maximization and value iteration\nDense and sparse matrix support\nParametric probability types for customizable precision\nMultithreaded CPU and CUDA-accelerated value iteration\nData loading and writing in formats by various tools (PRISM, bmdp-tool, IMDP.jl)","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Info\nUntil now, all state-of-the-art tools for IMDPs have been standalone programs.  This is explicitly a package, enabling better integration with other tools and libraries.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package requires Julia v1.9 or later. Refer to the official documentation on how to install it for your system.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To install IMDP.jl, use the following command inside Julia's REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"IMDP\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to use the CUDA extension, you also need to install CUDA.jl:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> import Pkg; Pkg.add(\"CUDA\")","category":"page"},{"location":"reference/systems/#System-representation","page":"Systems","title":"System representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovProcess","category":"page"},{"location":"reference/systems/#IMDP.IntervalMarkovProcess","page":"Systems","title":"IMDP.IntervalMarkovProcess","text":"IntervalMarkovProcess\n\nAn abstract type for interval Markov processes including IntervalMarkovChain and IntervalMarkovDecisionProcess.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#Markov-chain","page":"Systems","title":"Markov chain","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovChain\ntransition_prob(mc::IntervalMarkovChain)\nnum_states(mc::IntervalMarkovChain)\ninitial_states(mc::IntervalMarkovChain)","category":"page"},{"location":"reference/systems/#IMDP.IntervalMarkovChain","page":"Systems","title":"IMDP.IntervalMarkovChain","text":"IntervalMarkovChain{P <: IntervalProbabilities, T <: Integer, VT <: AbstractVector{T}}\n\nA type representing Interval Markov Chains (IMC), which are Markov Chains with uncertainty in the form of intervals on the transition probabilities.\n\nFormally, let (S S_0 barP underbarP) be an interval Markov chain, where S is the set of states, S_0 subset S is a set of initial states, and barP  mathbbR^S times S and underbarP  mathbbR^S times S are the upper and lower bound transition probability matrices prespectively. Then the IntervalMarkovChain type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, and initial_states is the set of initial state S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\ntransition_prob::P: interval on transition probabilities.\ninitial_states::VT: initial states.\nnum_states::T: number of states.\n\nExamples\n\nprob = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.0\n        0.1 0.3 0.0\n        0.2 0.1 1.0\n    ],\n    upper = [\n        0.5 0.7 0.0\n        0.6 0.5 0.0\n        0.7 0.3 1.0\n    ],\n)\n\nmc = IntervalMarkovChain(prob)\n# or\ninitial_states = [1, 2, 3]\nmc = IntervalMarkovChain(prob, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IMDP.transition_prob-Tuple{IntervalMarkovChain}","page":"Systems","title":"IMDP.transition_prob","text":"transition_prob(s::IntervalMarkovChain)\n\nReturn the interval on transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.num_states-Tuple{IntervalMarkovChain}","page":"Systems","title":"IMDP.num_states","text":"num_states(s::IntervalMarkovChain)\n\nReturn the number of states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.initial_states-Tuple{IntervalMarkovChain}","page":"Systems","title":"IMDP.initial_states","text":"initial_states(s::IntervalMarkovChain)\n\nReturn the initial states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Markov-decision-process","page":"Systems","title":"Markov decision process","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalMarkovDecisionProcess\ntransition_prob(mdp::IntervalMarkovDecisionProcess)\nnum_states(mdp::IntervalMarkovDecisionProcess)\ninitial_states(mdp::IntervalMarkovDecisionProcess)\nactions(mdp::IntervalMarkovDecisionProcess)\nnum_choices(mdp::IntervalMarkovDecisionProcess)","category":"page"},{"location":"reference/systems/#IMDP.IntervalMarkovDecisionProcess","page":"Systems","title":"IMDP.IntervalMarkovDecisionProcess","text":"IntervalMarkovDecisionProcess{\n    P <: IntervalProbabilities,\n    T <: Integer,\n    VT <: AbstractVector{T},\n    VI <: AbstractVector{T},\n    VA <: AbstractVector,\n}\n\nA type representing Interval Markov Decision Processes (IMDP), which are Markov Decision Processes with uncertainty in the form of intervals on the transition probabilities.\n\nFormally, let (S S_0 A barP underbarP) be an interval Markov decision processes, where S is the set of states, S_0 subset S is the set of initial states, A is the set of actions, and barP  A to mathbbR^S times S and underbarP  A to mathbbR^S times S are functions representing the upper and lower bound transition probability matrices prespectively for each action. Then the IntervalMarkovDecisionProcess type is defined as follows: indices 1:num_states are the states in S, transition_prob represents barP and underbarP, action_vals contains the actions available in each state, and initial_states is the set of initial states S_0. If no initial states are specified, then the initial states are assumed to be all states in S.\n\nFields\n\ntransition_prob::P: interval on transition probabilities where columns represent source/action pairs and rows represent target states.\nstateptr::VT: pointer to the start of each source state in transition_prob (i.e. transition_prob[:, stateptr[j]:stateptr[j + 1] - 1] is the transition   probability matrix for source state j) in the style of colptr for sparse matrices in CSC format.\naction_vals::VA: actions available in each state. Can be any eltype.\ninitial_states::VI: initial states.\nnum_states::T: number of states.\n\nExamples\n\ntransition_probs = IntervalProbabilities(;\n    lower = [\n        0.0 0.5 0.1 0.2 0.0\n        0.1 0.3 0.2 0.3 0.0\n        0.2 0.1 0.3 0.4 1.0\n    ],\n    upper = [\n        0.5 0.7 0.6 0.6 0.0\n        0.6 0.5 0.5 0.5 0.0\n        0.7 0.3 0.4 0.4 1.0\n    ],\n)\n\nstateptr = [1, 3, 5, 6]\nactions = [\"a1\", \"a2\", \"a1\", \"a2\", \"sinking\"]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, stateptr, actions, initial_states)\n\nThere is also a constructor for IntervalMarkovDecisionProcess where the transition probabilities are given as a list of  mappings from actions to transition probabilities for each source state.\n\nprob1 = IntervalProbabilities(;\n    lower = [\n        0.0 0.5\n        0.1 0.3\n        0.2 0.1\n    ],\n    upper = [\n        0.5 0.7\n        0.6 0.5\n        0.7 0.3\n    ],\n)\n\nprob2 = IntervalProbabilities(;\n    lower = [\n        0.1 0.2\n        0.2 0.3\n        0.3 0.4\n    ],\n    upper = [\n        0.6 0.6\n        0.5 0.5\n        0.4 0.4\n    ],\n)\n\nprob3 = IntervalProbabilities(;\n    lower = [0.0; 0.0; 1.0],\n    upper = [0.0; 0.0; 1.0]\n)\n\ntransition_probs = [[\"a1\", \"a2\"] => prob1, [\"a1\", \"a2\"] => prob2, [\"sinking\"] => prob3]\ninitial_states = [1]\n\nmdp = IntervalMarkovDecisionProcess(transition_probs, initial_states)\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IMDP.transition_prob-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IMDP.transition_prob","text":"transition_prob(s::IntervalMarkovDecisionProcess)\n\nReturn the interval on transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.num_states-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IMDP.num_states","text":"num_states(s::IntervalMarkovDecisionProcess)\n\nReturn the number of states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.initial_states-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IMDP.initial_states","text":"initial_states(s::IntervalMarkovDecisionProcess)\n\nReturn the initial states.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.actions-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IMDP.actions","text":"actions(s::IntervalMarkovDecisionProcess)\n\nReturn a vector of actions (choices in PRISM terminology).\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.num_choices-Tuple{IntervalMarkovDecisionProcess}","page":"Systems","title":"IMDP.num_choices","text":"num_choices(s::IntervalMarkovDecisionProcess)\n\nReturn the sum of the number of actions available in each state sum_j mathrmnum_actions(s_j).\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#Probability-representation","page":"Systems","title":"Probability representation","text":"","category":"section"},{"location":"reference/systems/","page":"Systems","title":"Systems","text":"IntervalProbabilities\nlower(s::IntervalProbabilities)\nupper(s::IntervalProbabilities)\ngap(s::IntervalProbabilities)\nsum_lower(s::IntervalProbabilities)\nnum_source(s::IntervalProbabilities)\nnum_target(s::IntervalProbabilities)","category":"page"},{"location":"reference/systems/#IMDP.IntervalProbabilities","page":"Systems","title":"IMDP.IntervalProbabilities","text":"IntervalProbabilities{R, VR <: AbstractVector{R}, MR <: AbstractMatrix{R}}\n\nA matrix pair to represent the lower and upper bound transition probabilities from a source state or source/action pair to a target state. The matrices can be Matrix{R} or SparseMatrixCSC{R}, or their CUDA equivalents. For memory efficiency, it is recommended to use sparse matrices.\n\nThe columns represent the source and the rows represent the target, as if the probability matrix was a linear transformation. Mathematically, let P be the probability matrix. Then P_ij represents the probability of transitioning from state j (or with state/action pair j) to state i. Due to the column-major format of Julia, this is also a more efficient representation (in terms of cache locality).\n\nThe lower bound is explicitly stored, while the upper bound is computed from the lower bound and the gap. This choice is  because it simplifies repeated probability assignment using O-maximization [1].\n\nFields\n\nlower::MR: The lower bound transition probabilities from a source state or source/action pair to a target state.\ngap::MR: The gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\nsum_lower::VR: The sum of lower bound transition probabilities from a source state or source/action pair to all target states.\n\nExamples\n\ndense_prob = IntervalProbabilities(;\n    lower = [0.0 0.5; 0.1 0.3; 0.2 0.1],\n    upper = [0.5 0.7; 0.6 0.5; 0.7 0.3],\n)\n\nsparse_prob = IntervalProbabilities(;\n    lower = sparse_hcat(\n        SparseVector(15, [4, 10], [0.1, 0.2]),\n        SparseVector(15, [5, 6, 7], [0.5, 0.3, 0.1]),\n    ),\n    upper = sparse_hcat(\n        SparseVector(15, [1, 4, 10], [0.5, 0.6, 0.7]),\n        SparseVector(15, [5, 6, 7], [0.7, 0.5, 0.3]),\n    ),\n)\n\n[1] M. Lahijanian, S. B. Andersson and C. Belta, \"Formal Verification and Synthesis for Discrete-Time Stochastic Systems,\" in IEEE Transactions on Automatic Control, vol. 60, no. 8, pp. 2031-2045, Aug. 2015, doi: 10.1109/TAC.2015.2398883.\n\n\n\n\n\n","category":"type"},{"location":"reference/systems/#IMDP.lower-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.lower","text":"lower(s::IntervalProbabilities)\n\nReturn the lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.upper-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.upper","text":"upper(s::IntervalProbabilities)\n\nReturn the upper bound transition probabilities from a source state or source/action pair to a target state.\n\nnote: Note\nIt is not recommended to use this function for the hot loop of O-maximization, because it is not just an accessor and requires allocation and computation.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.gap-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.gap","text":"gap(s::IntervalProbabilities)\n\nReturn the gap between upper and lower bound transition probabilities from a source state or source/action pair to a target state.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.sum_lower-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.sum_lower","text":"sum_lower(s::IntervalProbabilities)\n\nReturn the sum of lower bound transition probabilities from a source state or source/action pair to all target states. This is useful in efficiently implementing O-maximization, where we start with a lower bound probability assignment and iteratively, according to the ordering, adding the gap until the sum of probabilities is 1.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.num_source-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.num_source","text":"num_source(s::IntervalProbabilities)\n\nReturn the number of source states or source/action pairs.\n\n\n\n\n\n","category":"method"},{"location":"reference/systems/#IMDP.num_target-Tuple{IntervalProbabilities}","page":"Systems","title":"IMDP.num_target","text":"num_target(s::IntervalProbabilities)\n\nReturn the number of target states.\n\n\n\n\n\n","category":"method"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"}]
}
